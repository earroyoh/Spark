{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%scala\n",
    "\n",
    "val flightData2015 = spark.read.option(\"inferSchema\", \"true\").option(\"header\", \"true\").csv(\"/tmp/data/flight-data/csv/2015-summary.csv\")\n",
    "flightData2015.take(3)\n",
    "flightData2015.sort(\"count\").explain()\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"5\")\n",
    "flightData2015.sort(\"count\").take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flightData2015.createOrReplaceTempView(\"flight_data_2015\")\n",
    "val sqlWay = spark.sql(\"\"\"SELECT DEST_COUNTRY_NAME, count(1) FROM flight_data_2015 GROUP BY DEST_COUNTRY_NAME\"\"\")\n",
    "val maxSql = spark.sql(\"\"\"SELECT DEST_COUNTRY_NAME, sum(count) as destination_total FROM flight_data_2015 GROUP BY DEST_COUNTRY_NAME ORDER BY sum(count) DESC LIMIT 5\"\"\")\n",
    "case class Flight(DEST_COUNTRY_NAME: String, ORIGIN_COUNTRY_NAME: String, count:BigInt)\n",
    "val flightsDF = spark.read.parquet(\"/tmp/data/flight-data/parquet/2010-summary.parquet/\")\n",
    "val flights = flightsDF.as[Flight]\n",
    "flights.filter(flight_row => flight_row.ORIGIN_COUNTRY_NAME != \"Canada\").take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val staticDataFrame = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load(\"/tmp/data/retail-data/by-day/*.csv\")\n",
    "staticDataFrame.createOrReplaceTempView(\"retail_data\")\n",
    "val staticSchema = staticDataFrame.schema\n",
    "\n",
    "import org.apache.spark.sql.functions.{window, column, desc, col}\n",
    "staticDataFrame\n",
    ".selectExpr(\n",
    "\"CustomerId\",\n",
    "\"(UnitPrice * Quantity) as total_cost\",\n",
    "\"InvoiceDate\")\n",
    ".groupBy(\n",
    "col(\"CustomerId\"), window(col(\"InvoiceDate\"), \"1 day\"))\n",
    ".sum(\"total_cost\")\n",
    ".orderBy(desc(\"sum(total_cost)\"))\n",
    "\n",
    "val streamingDataFrame = spark.readStream.schema(staticSchema).option(\"maxFilesPerTrigger\", 1).format(\"csv\").option(\"header\", \"true\").load(\"/tmp/data/retail-data/by-day/*.csv\")\n",
    "val purchaseByCustomerPerHour = streamingDataFrame.selectExpr(\"CustomerId\",\"(UnitPrice * Quantity) as total_cost\",\"InvoiceDate\").groupBy($\"CustomerId\", window($\"InvoiceDate\", \"1 day\")).sum(\"total_cost\")\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"5\")\n",
    "\n",
    "purchaseByCustomerPerHour.writeStream.format(\"memory\").queryName(\"customer_purchases\").outputMode(\"complete\").start()\n",
    "purchaseByCustomerPerHour.writeStream.format(\"console\").queryName(\"customer_purchases\").outputMode(\"complete\").start()\n",
    "spark.sql(\"\"\"SELECT * FROM customer_purchases ORDER BY `sum(total_cost)` DESC\"\"\").take(5)\n",
    "\n",
    "staticDataFrame.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions.date_format\n",
    "val preppedDataFrame = staticDataFrame.na.fill(0).withColumn(\"day_of_week\", date_format($\"InvoiceDate\", \"EEEE\")).coalesce(5)\n",
    "val trainDataFrame = preppedDataFrame.where(\"InvoiceDate < '2011-07-01'\")\n",
    "val testDataFrame = preppedDataFrame.where(\"InvoiceDate >= '2011-07-01'\")\n",
    "trainDataFrame.count()\n",
    "testDataFrame.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.ml.feature.StringIndexer\n",
    "val indexer = new StringIndexer().setInputCol(\"day_of_week\").setOutputCol(\"day_of_week_index\")\n",
    "import org.apache.spark.ml.feature.OneHotEncoder\n",
    "val encoder = new OneHotEncoder().setInputCol(\"day_of_week_index\").setOutputCol(\"day_of_week_encoded\")\n",
    "\n",
    "import org.apache.spark.ml.feature.VectorAssembler\n",
    "val vectorAssembler = new VectorAssembler().setInputCols(Array(\"UnitPrice\", \"Quantity\", \"day_of_week_encoded\")).setOutputCol(\"features\")\n",
    "\n",
    "import org.apache.spark.ml.Pipeline\n",
    "val transformationPipeline = new Pipeline().setStages(Array(indexer, encoder, vectorAssembler))\n",
    "val fittedPipeline = transformationPipeline.fit(trainDataFrame)\n",
    "val transformedTraining = fittedPipeline.transform(trainDataFrame)\n",
    "transformedTraining.cache()\n",
    "\n",
    "import org.apache.spark.ml.clustering.KMeans\n",
    "val kmeans = new KMeans().setK(20).setSeed(1L)\n",
    "val kmModel = kmeans.fit(transformedTraining)\n",
    "kmModel.computeCost(transformedTraining)\n",
    "\n",
    "val transformedTest = fittedPipeline.transform(testDataFrame)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
